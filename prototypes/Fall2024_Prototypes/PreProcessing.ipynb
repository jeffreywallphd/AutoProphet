{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5b2890-e208-4f36-a2b8-2788a7e0cd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        accessionNumber  filingDate  reportDate        acceptanceDateTime act  \\\n",
      "0  0001958244-24-005135  2024-10-04              2024-10-04T17:04:02.000Z  33   \n",
      "1  0000320193-24-000112  2024-10-03  2024-10-01  2024-10-03T18:31:01.000Z       \n",
      "2  0000320193-24-000111  2024-10-03  2024-10-01  2024-10-03T18:30:50.000Z       \n",
      "3  0000320193-24-000110  2024-10-03  2024-10-01  2024-10-03T18:30:40.000Z       \n",
      "4  0000320193-24-000109  2024-10-03  2024-10-01  2024-10-03T18:30:30.000Z       \n",
      "\n",
      "  form fileNumber filmNumber items core_type   size  isXBRL  isInlineXBRL  \\\n",
      "0  144  001-36743  241355561             144   5409       0             0   \n",
      "1    4                                     4  15107       0             0   \n",
      "2    4                                     4  10988       0             0   \n",
      "3    4                                     4  20468       0             0   \n",
      "4    4                                     4  18949       0             0   \n",
      "\n",
      "                      primaryDocument primaryDocDescription  \n",
      "0           xsl144X01/primary_doc.xml                        \n",
      "1  xslF345X05/wk-form4_1727994654.xml                FORM 4  \n",
      "2  xslF345X05/wk-form4_1727994644.xml                FORM 4  \n",
      "3  xslF345X05/wk-form4_1727994634.xml                FORM 4  \n",
      "4  xslF345X05/wk-form4_1727994624.xml                FORM 4  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define SEC API URL (for example, Apple Inc.'s CIK: 0000320193)\n",
    "SEC_API_URL = \"https://data.sec.gov/submissions/CIK0000320193.json\"\n",
    "\n",
    "# Set your User-Agent (required by SEC)\n",
    "headers = {\n",
    "    'User-Agent': 'Hasibur Rashid Mahi hmahi@mtu.edu'\n",
    "}\n",
    "\n",
    "# Make the request to the SEC API\n",
    "response = requests.get(SEC_API_URL, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Access filings from the data\n",
    "    filings = data['filings']['recent']\n",
    "    \n",
    "    # Convert to DataFrame for better visualization or CSV export\n",
    "    filings_df = pd.DataFrame(filings)\n",
    "    \n",
    "    # Display the first few rows of the filings\n",
    "    print(filings_df.head())\n",
    "    \n",
    "    # Save the data to CSV if needed\n",
    "    filings_df.to_csv('sec_filings.csv', index=False)\n",
    "else:\n",
    "    print(f\"Error fetching data: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "157cd970-e92c-4a1d-8695-0a215a95f4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n",
      "Fetching page 6...\n",
      "Fetching page 7...\n",
      "Fetching page 8...\n",
      "Fetching page 9...\n",
      "Fetching page 10...\n",
      "Saved 10000 filings to sec_filings_10000.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define SEC API URL (for example, Apple Inc.'s CIK: 0000320193)\n",
    "SEC_API_URL = \"https://data.sec.gov/submissions/CIK0000320193.json\"\n",
    "\n",
    "# Set your User-Agent (required by SEC)\n",
    "headers = {\n",
    "    'User-Agent': 'Hasibur Rashid Mahi hmahi@mtu.edu'\n",
    "}\n",
    "\n",
    "# Function to fetch filings data and save it to CSV\n",
    "def fetch_sec_data(url, headers, output_csv, target_count=10000):\n",
    "    # Initialize an empty DataFrame to store all filings\n",
    "    all_filings_df = pd.DataFrame()\n",
    "\n",
    "    # Keep track of how many filings have been fetched so far\n",
    "    fetched_count = 0\n",
    "    page = 0\n",
    "\n",
    "    while fetched_count < target_count:\n",
    "        print(f\"Fetching page {page + 1}...\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the JSON response\n",
    "            data = response.json()\n",
    "\n",
    "            # Access filings from the data\n",
    "            filings = data['filings']['recent']\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            filings_df = pd.DataFrame(filings)\n",
    "\n",
    "            # Append to the full DataFrame\n",
    "            all_filings_df = pd.concat([all_filings_df, filings_df], ignore_index=True)\n",
    "            \n",
    "            # Update the count and page for fetching data\n",
    "            fetched_count = len(all_filings_df)\n",
    "            page += 1\n",
    "\n",
    "            # Break if we have enough data\n",
    "            if fetched_count >= target_count:\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Error fetching data: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        # Sleep for a while to avoid hitting the SEC rate limit\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Truncate to the target count if necessary\n",
    "    all_filings_df = all_filings_df.head(target_count)\n",
    "\n",
    "    # Save the data to CSV\n",
    "    all_filings_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved {len(all_filings_df)} filings to {output_csv}\")\n",
    "\n",
    "# Fetch and save the dataset\n",
    "fetch_sec_data(SEC_API_URL, headers, 'sec_filings_10000.csv', target_count=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "650eb809-0617-4a31-a450-7a6f4e02aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Step 1: Load the SEC dataset\n",
    "data = pd.read_csv('sec_filings_10000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f93eaa14-dc46-4ac3-952a-c65e2d222b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Handle Missing Values\n",
    "# Fill missing values with forward fill method initially, then use KNN imputer for critical columns\n",
    "critical_columns = ['revenue', 'assets', 'netIncome']  # Example critical columns\n",
    "\n",
    "# Forward fill for general missing values\n",
    "data = data.ffill()\n",
    "\n",
    "# Check if critical columns exist in the dataset before applying KNN Imputer\n",
    "existing_critical_columns = [col for col in critical_columns if col in data.columns]\n",
    "if existing_critical_columns:\n",
    "    # KNN Imputer for critical columns\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    data[existing_critical_columns] = imputer.fit_transform(data[existing_critical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3999570d-5ba9-42e6-bcf6-604c1d276bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Parse Dates\n",
    "# Convert filingDate to datetime object if available\n",
    "if 'filingDate' in data.columns:\n",
    "    data['filingDate'] = pd.to_datetime(data['filingDate'], errors='coerce')\n",
    "\n",
    "    # Drop rows with invalid dates\n",
    "    data.dropna(subset=['filingDate'], inplace=True)\n",
    "\n",
    "    # Create new time-based features\n",
    "    data['year'] = data['filingDate'].dt.year\n",
    "    data['month'] = data['filingDate'].dt.month\n",
    "    data['quarter'] = data['filingDate'].dt.quarter\n",
    "    data['day_of_week'] = data['filingDate'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c99a7f3-2f1a-4686-9837-84a0ca33e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Set filingDate as Index\n",
    "# Sort the data by filingDate and set it as index for time series analysis\n",
    "if 'filingDate' in data.columns:\n",
    "    data.set_index('filingDate', inplace=True)\n",
    "    data.sort_index(inplace=True)\n",
    "\n",
    "# Step 5: Handle Categorical Variables\n",
    "# Convert form type to dummy variables if it exists in the dataset\n",
    "if 'form' in data.columns:\n",
    "    data = pd.get_dummies(data, columns=['form'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c458d4c-2828-4ee1-8675-1084e280184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Outlier Detection and Treatment\n",
    "# Use Z-score to identify and cap outliers for numerical features\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numeric_features:\n",
    "    data[col] = np.where(np.abs(zscore(data[col])) > 3, data[col].median(), data[col])\n",
    "\n",
    "# Step 7: Feature Engineering\n",
    "# Create rolling features and lag features for time series analysis\n",
    "rolling_window = 3  # Example 3-month rolling window\n",
    "for col in existing_critical_columns:\n",
    "    data[f'{col}_rolling_mean'] = data[col].rolling(window=rolling_window).mean()\n",
    "    data[f'{col}_lag_1'] = data[col].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3806c02-f2c7-41bf-9a89-4e73d4e85be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Stationarity Check and Transformation\n",
    "# Check for stationarity using Augmented Dickey-Fuller test if 'revenue' column exists\n",
    "if 'revenue' in data.columns:\n",
    "    adf_result = adfuller(data['revenue'].dropna())\n",
    "    if adf_result[1] > 0.05:\n",
    "        # If p-value > 0.05, the series is non-stationary; apply differencing\n",
    "        data['revenue_diff'] = data['revenue'].diff().dropna()\n",
    "\n",
    "# Step 9: Interaction Features\n",
    "# Create interaction features between multiple financial metrics if columns exist\n",
    "if 'revenue' in data.columns and 'assets' in data.columns:\n",
    "    data['revenue_assets_ratio'] = data['revenue'] / data['assets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f61266d-3e34-4e9f-9393-66d5e6b92415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed and saved to sec_filings_10000_preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Feature Scaling\n",
    "# Identify numerical features for scaling\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = StandardScaler()\n",
    "data[numeric_features] = scaler.fit_transform(data[numeric_features])\n",
    "\n",
    "# Step 11: Save the Preprocessed Dataset\n",
    "data.to_csv('sec_filings_10000_preprocessed.csv')\n",
    "print(\"Preprocessing completed and saved to sec_filings_10000_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4018380-e1ed-4204-b462-7cf5547d41a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
